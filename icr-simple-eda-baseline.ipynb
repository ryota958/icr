{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* 18.05: Updated competition metric","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\nimport warnings\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom tqdm.auto import tqdm\nimport catboost as cb\n\n\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings('ignore')\n\nplt.style.use('ggplot')\ncust_color = [\n    '#EDC7B7',\n    '#EEE2DC',\n    '#BAB2B5',\n    '#123C69',\n    '#AC3B61'\n]\nplt.rcParams['figure.figsize'] = (12,4)\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams[\"axes.grid\"] = False\nplt.rcParams[\"grid.color\"] = cust_color[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\nplt.rcParams[\"font.family\"] = \"monospace\"\n\nplt.rcParams['axes.edgecolor'] = 'black'\nplt.rcParams['figure.frameon'] = True\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.bottom'] = True\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.linewidth'] = 0.5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-17T06:32:22.081829Z","iopub.execute_input":"2023-06-17T06:32:22.082197Z","iopub.status.idle":"2023-06-17T06:32:23.225827Z","shell.execute_reply.started":"2023-06-17T06:32:22.082168Z","shell.execute_reply":"2023-06-17T06:32:23.224783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\ngreeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n\ntrain.columns = train.columns.str.strip()\ntest.columns = test.columns.str.strip()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:23.228030Z","iopub.execute_input":"2023-06-17T06:32:23.228491Z","iopub.status.idle":"2023-06-17T06:32:23.273737Z","shell.execute_reply.started":"2023-06-17T06:32:23.228453Z","shell.execute_reply":"2023-06-17T06:32:23.272932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = train.columns.tolist()[1:-1]\ncat_cols = 'EJ'\nnum_cols.remove(cat_cols)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:23.275140Z","iopub.execute_input":"2023-06-17T06:32:23.275756Z","iopub.status.idle":"2023-06-17T06:32:23.280836Z","shell.execute_reply.started":"2023-06-17T06:32:23.275720Z","shell.execute_reply":"2023-06-17T06:32:23.279829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features of Interest","metadata":{}},{"cell_type":"code","source":"features_std = train.loc[:,num_cols].apply(lambda x: np.std(x)).sort_values(\n    ascending=False)\nf_std = train[features_std.iloc[:20].index.tolist()]\n\nwith pd.option_context('mode.use_inf_as_na', True):\n    features_skew = np.abs(train.loc[:,num_cols].apply(lambda x: np.abs(skew(x))).sort_values(\n        ascending=False)).dropna()\nskewed = train[features_skew.iloc[:20].index.tolist()]\n\nwith pd.option_context('mode.use_inf_as_na', True):\n    features_kurt = np.abs(train.loc[:,num_cols].apply(lambda x: np.abs(kurtosis(x))).sort_values(\n        ascending=False)).dropna()\nkurt_f = train[features_kurt.iloc[:20].index.tolist()]","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:23.328690Z","iopub.execute_input":"2023-06-17T06:32:23.329055Z","iopub.status.idle":"2023-06-17T06:32:23.381599Z","shell.execute_reply.started":"2023-06-17T06:32:23.329028Z","shell.execute_reply":"2023-06-17T06:32:23.380538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feat_dist(df, cols, rows=3, columns=3, title=None, figsize=(30, 25)):\n    \n    fig, axes = plt.subplots(rows, columns, figsize=figsize, constrained_layout=True)\n    axes = axes.flatten()\n\n    for i, j in zip(cols, axes):\n        sns.kdeplot(df, x=i, ax=j, hue='Class', linewidth=1.5, linestyle='--')\n        \n        (mu, sigma) = norm.fit(df[i])\n        \n        xmin, xmax = j.get_xlim()[0], j.get_xlim()[1]\n        x = np.linspace(xmin, xmax, 100)\n        p = norm.pdf(x, mu, sigma)\n        j.plot(x, p, 'k', linewidth=2)\n        \n        j.set_title('Dist of {0} Norm Fit: $\\mu=${1:.2g}, $\\sigma=${2:.2f}'.format(i, mu, sigma), weight='bold')\n        j.legend(labels=[f'Class0_{i}', f'Class1_{i}', 'Normal Dist'])\n        fig.suptitle(f'{title}', fontsize=24, weight='bold')","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:23.593658Z","iopub.execute_input":"2023-06-17T06:32:23.594813Z","iopub.status.idle":"2023-06-17T06:32:23.602447Z","shell.execute_reply.started":"2023-06-17T06:32:23.594759Z","shell.execute_reply":"2023-06-17T06:32:23.601609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instead of creating a big blob with every feature I select some features with odd statistics","metadata":{}},{"cell_type":"code","source":"feat_dist(train, f_std.columns.tolist(), rows=2, columns=4, title='Distribution of High Std Features', figsize=(30, 8))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:24.110297Z","iopub.execute_input":"2023-06-17T06:32:24.111612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### You can notice pretty big difference between class 0 and class 1 distributions in odd features...","metadata":{}},{"cell_type":"code","source":"feat_dist(train, skewed.columns.tolist(), rows=2, columns=4, title='Distribution of Skewed Features', figsize=(30, 8))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:29.394300Z","iopub.execute_input":"2023-06-17T06:32:29.394876Z","iopub.status.idle":"2023-06-17T06:32:34.342888Z","shell.execute_reply.started":"2023-06-17T06:32:29.394844Z","shell.execute_reply":"2023-06-17T06:32:34.341923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar pattern here","metadata":{}},{"cell_type":"code","source":"feat_dist(train, kurt_f.columns.tolist(), rows=2, columns=4, title='Distribution of High Kurtosis Features', figsize=(30, 8))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:34.344159Z","iopub.execute_input":"2023-06-17T06:32:34.345128Z","iopub.status.idle":"2023-06-17T06:32:39.212075Z","shell.execute_reply.started":"2023-06-17T06:32:34.345091Z","shell.execute_reply":"2023-06-17T06:32:39.211176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at most correlated features with target","metadata":{}},{"cell_type":"code","source":"correlations = train.loc[:,num_cols].corrwith(train['Class']).to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(6,4))\nsns.heatmap(sorted_correlations.iloc[1:].to_frame()[sorted_correlations>=.15], cmap='inferno', annot=True, vmin=-1, vmax=1, ax=ax)\nplt.title('Feature Correlations With Target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:39.214164Z","iopub.execute_input":"2023-06-17T06:32:39.215009Z","iopub.status.idle":"2023-06-17T06:32:39.703653Z","shell.execute_reply.started":"2023-06-17T06:32:39.214977Z","shell.execute_reply":"2023-06-17T06:32:39.702624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations Between Features","metadata":{}},{"cell_type":"code","source":"correlations = train.loc[:,num_cols].corr().abs().unstack().sort_values(kind=\"quicksort\",ascending=False).reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] #preventing 1.0 corr\ncorr_max=correlations.level_0.head(150).tolist()\ncorr_max=list(set(corr_max)) #removing duplicates\n\ncorr_min=correlations.level_0.tail(34).tolist()\ncorr_min=list(set(corr_min)) #removing duplicates\n\n\ncorrelation_train = train.loc[:,corr_max].corr()\nmask = np.triu(correlation_train.corr())\n\nplt.figure(figsize=(30, 12))\nsns.heatmap(correlation_train,\n            mask=mask,\n            annot=True,\n            fmt='.3f',\n            cmap='coolwarm',\n            linewidths=0.00,\n            cbar=True)\n\n\nplt.suptitle('Features with Highest Correlations',  weight='bold')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:39.704896Z","iopub.execute_input":"2023-06-17T06:32:39.705221Z","iopub.status.idle":"2023-06-17T06:32:45.512154Z","shell.execute_reply.started":"2023-06-17T06:32:39.705194Z","shell.execute_reply":"2023-06-17T06:32:45.510842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see some correlations, let's take another look:","metadata":{}},{"cell_type":"code","source":"corr = train.loc[:, num_cols].corr()\nsns.clustermap(corr, metric=\"correlation\", cmap=\"hot\", figsize=(20, 20))\nplt.suptitle('Correlations Between Features', fontsize=24, weight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:45.513764Z","iopub.execute_input":"2023-06-17T06:32:45.514659Z","iopub.status.idle":"2023-06-17T06:32:49.768749Z","shell.execute_reply.started":"2023-06-17T06:32:45.514621Z","shell.execute_reply":"2023-06-17T06:32:49.767669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Yes, there are definitely some strong correlated features. We should examine them further before making any conclusions.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(12,4), constrained_layout=True)\naxes = axes.flatten()\n\n# for i, j in zip(cols, axes):\ni = 0\nfor row in range(0,16,2):\n    a = correlations.reset_index(drop=True).loc[row, ['level_0', 'level_1']][0]\n    b = correlations.reset_index(drop=True).loc[row, ['level_0', 'level_1']][1]    \n   \n    sns.regplot(train, x=a, y=b, ci=False, ax=axes[i], order=1, scatter_kws={'color':'red', 's':1.5}, line_kws={'color':'black', 'linewidth':1.5})\n    i+=1\n    \nplt.suptitle('Highly Correlated Features',  weight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:52.063131Z","iopub.execute_input":"2023-06-17T06:32:52.063438Z","iopub.status.idle":"2023-06-17T06:32:54.787892Z","shell.execute_reply.started":"2023-06-17T06:32:52.063411Z","shell.execute_reply":"2023-06-17T06:32:54.787127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some of these strongly correlated features are affected by outliers or influential points. Perhaps a more robust approach would make more sense.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(12,4), constrained_layout=True)\naxes = axes.flatten()\n\n# for i, j in zip(cols, axes):\ni = 0\nfor row in range(0,16,2):\n    a = correlations.reset_index(drop=True).loc[row, ['level_0', 'level_1']][0]\n    b = correlations.reset_index(drop=True).loc[row, ['level_0', 'level_1']][1]\n    \n   \n    sns.regplot(train, x=a, y=b, ci=False, ax=axes[i], robust=True, scatter_kws={'color':'red', 's':1.5}, line_kws={'color':'black', 'linewidth':1.5})\n    i+=1\n    \nplt.suptitle('Highly Correlated Features',  weight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:32:54.789041Z","iopub.execute_input":"2023-06-17T06:32:54.789477Z","iopub.status.idle":"2023-06-17T06:34:58.804969Z","shell.execute_reply.started":"2023-06-17T06:32:54.789451Z","shell.execute_reply":"2023-06-17T06:34:58.803829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indeed, looks like some strong correlations are not as strong in practice, but we still see some retaining a degree of importance.","metadata":{}},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"markdown","source":"### This is the simplest form of baseline approach I implemented in the first place. Using class weights seems to help. Using \"greeks.csv\" data to stratify observations helped a little with variance between folds in my case...","metadata":{}},{"cell_type":"code","source":"# Create a LabelEncoder object.\nencoder = LabelEncoder()\n# Transform the data.\ntrain[cat_cols] = encoder.fit_transform(train[cat_cols])\ntest[cat_cols] = encoder.transform(test[cat_cols])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:34:58.807669Z","iopub.execute_input":"2023-06-17T06:34:58.808217Z","iopub.status.idle":"2023-06-17T06:34:58.814728Z","shell.execute_reply.started":"2023-06-17T06:34:58.808184Z","shell.execute_reply":"2023-06-17T06:34:58.813683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = np.zeros((len(train), 2))\n\nskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfinal_preds = []\n\nparams={\n    'iterations':10000,\n    'learning_rate':0.005,\n    'early_stopping_rounds':1000,\n    'auto_class_weights':'Balanced',\n    'loss_function':'MultiClass',\n    'eval_metric':'MultiClass:use_weights=True',\n    'random_seed':42,\n    'use_best_model':True,\n    'l2_leaf_reg':1,\n    'max_ctr_complexity':15,\n    'max_depth':11, #元は10\n    \"grow_policy\":'Lossguide',\n    'max_leaves':64,\n    \"min_data_in_leaf\":40,\n\n    }\n\nfor train_index,val_index in skf.split(train, greeks.iloc[:,1:-1]):\n\n    X_train, X_val = train.loc[train_index, num_cols + [cat_cols]], train.loc[val_index, num_cols + [cat_cols]]\n    y_train, y_val = train.loc[train_index, 'Class'], train.loc[val_index, 'Class']\n    \n    \n    model = cb.CatBoostClassifier(**params)\n    model.fit(X_train,y_train,eval_set=[(X_val,y_val)], verbose=1000)\n    preds = model.predict_proba(X_val)\n    oof[val_index, :] = preds\n    final_preds.append(model.predict_proba(test.iloc[:,1:]))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:34:58.816019Z","iopub.execute_input":"2023-06-17T06:34:58.816494Z","iopub.status.idle":"2023-06-17T06:37:44.703594Z","shell.execute_reply.started":"2023-06-17T06:34:58.816468Z","shell.execute_reply":"2023-06-17T06:37:44.702780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" This is my quick draft implementation of the competition metric. I am not sure if it's working as intended but seems to be in line with other metrics. But anyways take it with a grain of salt, please let me know if there's something wrong...","metadata":{}},{"cell_type":"markdown","source":"Updated the metric based on this [discussion](https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/410864). It seems to get CV and LB closer, but correlation still similar with old implementation:\n\n![](https://i.imgur.com/4FcxvaO.png)","metadata":{}},{"cell_type":"code","source":"def balance_logloss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    y_pred / np.sum(y_pred, axis=1)[:, None]\n    nc = np.bincount(y_true)\n    \n    logloss = (-1/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(y_pred[:,0]))) - 1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred[:,1])))) / 2\n    \n    return logloss\n\nbalance_logloss(train['Class'], oof)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:37:44.705176Z","iopub.execute_input":"2023-06-17T06:37:44.705555Z","iopub.status.idle":"2023-06-17T06:37:44.716392Z","shell.execute_reply.started":"2023-06-17T06:37:44.705519Z","shell.execute_reply":"2023-06-17T06:37:44.715228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also use something like this for sanity check, seems reproducing similar scores:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\ndef balance_loglossv2(y_true, y_pred):\n    target_mean = y_true.mean()\n    w0 = 1/(1-target_mean)\n    w1 = 1/target_mean\n    sample_weight = [w0 if y == 0 else w1 for y in y_true]\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    \n    return loss\n\nprint(balance_loglossv2(train['Class'], oof))  ","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:37:44.717599Z","iopub.execute_input":"2023-06-17T06:37:44.717955Z","iopub.status.idle":"2023-06-17T06:37:44.732399Z","shell.execute_reply.started":"2023-06-17T06:37:44.717927Z","shell.execute_reply":"2023-06-17T06:37:44.731351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\nsample_submission[['class_0','class_1']] = np.mean(final_preds, axis=0)\nsample_submission.to_csv('submission.csv',index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2023-06-17T06:37:44.733603Z","iopub.execute_input":"2023-06-17T06:37:44.733977Z","iopub.status.idle":"2023-06-17T06:37:44.764894Z","shell.execute_reply.started":"2023-06-17T06:37:44.733951Z","shell.execute_reply":"2023-06-17T06:37:44.763970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thanks!","metadata":{}}]}